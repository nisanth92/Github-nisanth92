---
title: "HW 3"
author: "Nisanth Dheram"
date: "11-8-2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=T)
library(magrittr) 
library(dplyr)
library(ggplot2)
library(gridExtra)
library(stringr)
```

## Introduction

You have recently joined the data science team at Target, and your manager has come to you to help her solve an important (and high visibility) question to the firm. The Target.com and Advertising & Marketing groups have a joint initiative to better target customers with sponsored search advertising (also known as “paid search” or just “search advertising”) for products online. The teams cite search advertising as an important and effective marketing channel for Target.com (and the entire Target firm more broadly). The effectiveness of search advertising is attributed to the fact that search engines match the ads shown to a consumer with their current search intent derived from the keyword being used. It subsequently presents an appropriate list of ads based on factors such as bids placed by the advertisers (e.g., Target) and their historical performance. The ability to present consumers ads tailored to their search context (as indicated by the keywords) considerably increases the likelihood that they will click on one these ads. These teams at Target want to better understand how various features effect the performance of keyword, and are looking to you for guidance. Moreover, they are concerned that they have been following suboptimal bidding strategies by bidding too much on keyword terms that do not correspond to customers who are actually interested (or can be enticed toward) Target products. They fear that because the context of the consumer's search is not directly observable and its prediction can be nontrivial, they bidding strategy used by Target.com and the Advertising & Marketing groups could be placing too much value on keywords that are too ambiguous. This ambiguity results from the fact that the same keyword might refer to different contexts, and competing advertisers might have different intents while bidding on a particular keyword. Therefore, you are being asked to conduct an analysis to help provide insight into challenge for the teams.

## Goal 

The goal of this analysis is to explore how various features effect consumer search behavior. Above this, you will understand the interplay between a keyword’s context and consumers’ search behavior. More specifically, you will need to ascertain how the breadth of a keyword’s context might affect consumer behavior and keyword performance. In reality, keyword contextual ambiguity can result in both higher diversity in ad quality and higher probability of ad irrelevancy. Therefore, how keyword contextual ambiguity would affect consumer click behavior is unclear. To explore this question, you are going to use a rich dataset from a major search engine to perform a cross-category analysis and examine which of these two opposing effects dominates in the context of search advertising.

## Understanding the data 

The keyword level variables are in `keywords.csv`, with the following data dictionary

| Field | Description |
|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|
| num_ads | measures the total number of ads produced for a particular keyword | 
| num_clicks | measures the total number of clicks a particular keyword receives | 
| num_impressions | denotes the total number of times consumers search for a particular keyword in the dataset | 
| num_word | denotes the number of words in the keyword |
| brand | does the keyword refer to a specific brand |
| location | does the keyword refer to a specific location |
| log_trans | a measure of transactional intent, measured by the natural log of the frequency of transactional words that appear in the organic results for this keyword |
| avg_ad_quality | the average quality of ads shown for this keyword, where the quality of an ad is the average click through rate that the ad receives for other keywords |
| avg_num_ads | measures the average number of competing advertisers during an impression, which denotes the competitive intensity for a keyword |
|categoryid | id indicating the keyword's product category |

Additionally, the folder `organic_text` contains a file for each keyword. Each file contains the title and
textual content of the brief description of the top-50-ranked Google organic search results for the given keyword. This text is meant to be a reasonable approximation of text representing the contextual meaning(s) of each keyword.

Open the `keywords.csv` data in R 

```{r} 
folder="C:/Users/nisan/Desktop/My Coursework/Fall Semester/Exploratory Data Analytics/Homework 3"
setwd(folder) 
keywords<-read.csv("keywords.csv",header=TRUE,stringsAsFactors = FALSE)
```

## Exploration 

Using the skills you have amassed over the course, visualize and explore the relationship between the variables in your data and/or the keywords. Highlight and describe any interesting relationships or patterns you discover in the context of this problem. Feel free to transform or compute new variables. One variable you are required to create is click through rate (ctr) which is the proportion of ad impressions that result in actual clicks.

```{r}
keywords$ctr<-keywords$num_clicks/keywords$num_impressions
#keywords$adsperimp<-keywords$num_ads/keywords$num_impressions

keywords[,c("brand", "location","categoryid")] <- lapply(keywords[,c("brand", "location","categoryid")], factor)

str(keywords)

#Observing the variation of CTR across numeric features
p1<-ggplot(keywords, aes(num_ads, ctr)) +geom_point()
p2<-ggplot(keywords, aes(num_clicks, ctr)) +geom_point()
p3<-ggplot(keywords, aes(num_impressions, ctr)) +geom_point()
p4<-ggplot(keywords, aes(num_word, ctr)) +geom_point()
#grid.arrange( p1, p2,p3,p4,nrow=2 ,ncol=2)

p5<-ggplot(keywords, aes(log_trans, ctr)) +geom_point()
p6<-ggplot(keywords, aes(log_imp, ctr)) +geom_point()
p7<-ggplot(keywords, aes(avg_ad_quality, ctr)) +geom_point()
p8<-ggplot(keywords, aes(avg_num_ads, ctr)) +geom_point()
grid.arrange(p1, p2,p3,p4, p5, p6,p7,p8,nrow=3 ,ncol=3)

```


**We observe that features such as num_ads, num_clicks and avg_num_ads have a nearly logarithmic relationship with CTR. We can modify these variables when we regress them with CTR.**

**CTR seems to reduce with increase in num_ads but increases with increase in number of avg_num_ads.**


```{r} 
#Observing the trend of CTR with features across brand type
p11<-ggplot(keywords, aes(num_ads, ctr)) +geom_point()+facet_grid(~brand)
p12<-ggplot(keywords, aes(num_clicks, ctr)) +geom_point()+facet_grid(~brand)
p13<-ggplot(keywords, aes(num_impressions, ctr)) +geom_point()+facet_grid(~brand)
p14<-ggplot(keywords, aes(num_word, ctr)) +geom_point()+facet_grid(~brand)
#grid.arrange( p1, p2,p3,p4,nrow=2 ,ncol=2)

p15<-ggplot(keywords, aes(log_trans, ctr)) +geom_point()+facet_grid(~brand)
p16<-ggplot(keywords, aes(log_imp, ctr)) +geom_point()+facet_grid(~brand)
p17<-ggplot(keywords, aes(avg_ad_quality, ctr)) +geom_point()+facet_grid(~brand)
p18<-ggplot(keywords, aes(avg_num_ads, ctr)) +geom_point()+facet_grid(~brand)
grid.arrange(p11, p12,p13,p14, p15, p16,p17,p18,nrow=3 ,ncol=3,top ="CTR trends split by Brand type")

```


```{r} 
#Observing the trend of CTR with features across location type
p11<-ggplot(keywords, aes(num_ads, ctr)) +geom_point()+facet_grid(~location)
p12<-ggplot(keywords, aes(num_clicks, ctr)) +geom_point()+facet_grid(~location)
p13<-ggplot(keywords, aes(num_impressions, ctr)) +geom_point()+facet_grid(~location)
p14<-ggplot(keywords, aes(num_word, ctr)) +geom_point()+facet_grid(~location)
#grid.arrange( p1, p2,p3,p4,nrow=2 ,ncol=2)

p15<-ggplot(keywords, aes(log_trans, ctr)) +geom_point()+facet_grid(~location)
p16<-ggplot(keywords, aes(log_imp, ctr)) +geom_point()+facet_grid(~location)
p17<-ggplot(keywords, aes(avg_ad_quality, ctr)) +geom_point()+facet_grid(~location)
p18<-ggplot(keywords, aes(avg_num_ads, ctr)) +geom_point()+facet_grid(~location)
grid.arrange(p11, p12,p13,p14, p15, p16,p17,p18,nrow=3 ,ncol=3,top =" CTR trends split by Location type")

```

**The relationship of the features with CTR does not seem to vary much based on brand or location type of keywords.**

```{r, warning=FALSE}
#Observing the distribution of CTR across categories
ggplot(keywords, aes(categoryid,ctr))+geom_boxplot()+labs(title =" CTR across categories")

summary(keywords %>% filter(categoryid == 13))

ggplot(keywords, aes(categoryid,avg_ad_quality)) +geom_boxplot()+ labs(title =" Ad Quality across categories")
```

**Category13 has the highest average CTR which seems to be majorly due to the high avg_ad_quality as compared to other categories**

## Modeling 
The Target teams are concerned with understanding how click-through-rate (ctr) is affected by
other features in the `keyword.csv` dataset. Regress ctr on `num_ads`, `num_word`, `brand`, `location`,`log_trans`, `avg_ad_quality` and/or any other interactions or variables you created from your exploration.

```{r}
#Using the above observed trends to modify features and run linear regression with CTR

reg<-with(keywords,lm(ctr ~ log(num_ads)
                      +log_imp
                      +num_word
                      +brand
                      +location
                      +log_trans
                      +avg_ad_quality
                      +log(avg_num_ads)
                      +categoryid
                      ))
summary(reg)

```

```{r}
#Removing insignificant features and retaining only the significant features

reg<-with(keywords,lm(ctr ~ log(num_ads)
                      +log_imp
                      #+num_word
                      #+brand
                      #+location
                      +log_trans
                      +avg_ad_quality
                      +log(avg_num_ads)
                      +categoryid
                      ))
summary(reg)

```


**Many of these variables have a significant impact on CTR. Following is their individual impact for similar values of other variables:**

**num_ads: CTR increases by 0.69 for every 10% increase in num_ads.**

**num_imp: CTR reduces by 0.69 for every 10% increase in number of impressions.**

**avg_ad_quality: Not surprisingly, CTR increases by 2.13 for a unit increase in avg_ad_quality.**

**avg_num_ads: CTR reduces by 0.69 for every unit increase in the competetive intensity of a keyword.**

Turn categoryid into factors, if you have not already, and include this variable into your regression

**Adding categoryid as a predictor gives us a relative comparison of CTR between categories of keywords for similar values of other variables. For example categoryid1 has CTR of 0.03 more than category0**

## Topic Modeling 
One of the major questions of the Target teams is how a keyword’s context (and ambiguity
thereof) might affect consumer behavior and keyword performance. You will use the recently learned
algorithm Latent Dirchlet Allocation to discover topics and measure ambiguity.

```{r, include=FALSE}
# Here are the documentation for packages used in this code:
#https://cran.r-project.org/web/packages/tm/tm.pdf
library(tm)

#https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf
library(topicmodels)

# Use the SnowballC package to do stemming.
library(SnowballC) 
```

First you must pre-process the text before we run use LDA. 
```{r} 
dirname <- file.path(getwd(),"organic_text")
docs <- Corpus(DirSource(dirname, encoding = "UTF-8"))

# The following steps pre-process the raw text documents. 
# Remove punctuations and numbers because they are generally uninformative. 
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)

# Convert all words to lowercase. 
docs <- tm_map(docs, content_transformer(tolower))

# Remove stopwords such as "a", "the", etc. 
docs <- tm_map(docs, removeWords, stopwords("english"))

# Use the SnowballC package to do stemming. 
docs <- tm_map(docs, stemDocument)

# Remove excess white spaces between words. 
docs <- tm_map(docs, stripWhitespace)

# You can inspect the first document to see what it looks like with 
#docs[[1]]$content

# Convert all documents to a term frequency matrix. 
tfm <- DocumentTermMatrix(docs)

# We can check the dimension of this matrix by calling dim() 
#print(dim(tfm))
```

Now that we have finished pre-processing the text, we now can execute LDA to discover topics 
```{r} 
# we run LDA with 20 topics, and use Gibbs sampling as our method for identifying the optimal parameters 
# Note: this make take some time to run (~10 mins)
#results <- LDA(tfm, k = 20, method = "Gibbs", control = list(seed=1))
results <- LDA(tfm, k = 20, method = "Gibbs")
               
# Obtain the top w words (i.e., the w most probable words) for each topic, with the optional requirement that their probability is greater than thresh

#feel free to explore with different values of w and thresh
w=7
thresh = 0.010
#Terms <- terms(results, w) 
Terms <- terms(results, w,thresh) 
Terms
```

**Topic 1: Finance**

**Topic 2: Online games**

**Topic 3: Hotels**

**Topic 4: Wireless Communication**

**Topic 5: Lottery**

**Topic 6: Email**

**Topic 7: Software**

**Topic 8: Airine tickets**

**Topic 9: Logistics**

**Topic 10: Car Rentals**

**Topic 11: News**

**Topic 12: Maps**

**Topic 13: Home Shopping**

**Topic 14: Womens Shopping**

**Topic 15: Cars**

**Topic 16: Real Estate**

**Topic 17: Used Cars**

**Topic 18: Streaming Music**

**Topic 19: Job search**

**Topic 20: Yellow Pages**

```{r} 
# Obtain the most likely t topic assignments for each document. 
t=1 
Topic <- topics(results,t)

# Get the posterior probability for each document over each topic 
posterior <- posterior(results)[[2]]

# look at the posterior topic distribution for the dth document and plot it visually 
d = 1
posterior[d,]
barplot(posterior[d,])

# Examine the main topic for document d 
Terms[[which.max(posterior[d,])]]

# Compare the keyword of document d to the terms. 
#keywords$query[d]

```

**Keyword 1: Yellow Pages - Topic returned: Yellow pages - matches**

**Keyword 2: Kohls - Topic returned: Furniture Shopping - matches as people do search Kohls for furniture shopping**

**Keyword 3: Free Radio - Topic returned: Streaming Media - matches as people are most liekly looking for streaming radio options**    

## Keyword Ambiguity

Now that we have run LDA and are able to see the document distributions across topics, we want to use this to quantify the ambiguity of each keyword. We are going to use [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) to measure the ambiguity of a keyword:

\begin{equation*}
  Entropy(\text{keyword}_i) = - \sum_{t = 1}^{T}{p_{i,t} log(p_{i,t})}
\end{equation*}

where $p_{i,t}$ is the probability that the organic search text capturing the context of keyword $i$, is composed of topic $t$, and $T=20$ in our case of 20 topics. Write a general function to compute entropy, given a vector of probabilities. 
```{r} 
entropy <- function(probs)
{
entropy_var=0  
  for(i in probs)
  {
   entropy_var=entropy_var+(-i*log2(i))
  }
return(entropy_var)
} 
```

use this `entropy` function to generate a graph of entropy over the interval $[0,1]$.

```{r, warnings = FALSE}
p<-seq(0,1,by=0.01)
entropy_p<-c()

for(i in p)
{
x<-c(i,1-i)  
entropy_p<-append(entropy_p, entropy(x), after = length(entropy_p))
}

#plot(p,entropy_p)

ggplot(data=data.frame(p,entropy_p),aes(p,entropy_p)) +geom_point()

```

**Entropy captures the purity of data. As the graph shows, highly impure data with equal probabilities of 2 classes has higher entropy than when there is a dominance of one single class. In other words, highly pure data has low entropy**.

Create a new column in the keyword data that captures the entropy of each keyword 
```{r}
keyword_entropy<-apply(posterior,1,entropy)

querycode<-str_extract(rownames(posterior), "[0-9]+")

entropy_df<-data.frame(querycode,keyword_entropy)
rownames(entropy_df)<-NULL

keywords<-merge(keywords,entropy_df,by="querycode")

```

```{r}

ggplot(keywords, aes(keyword_entropy, ctr)) +geom_point()
```

Re-run the regressions from above, adding this new entropy measure as an additional independent variable
```{r}
reg<-with(keywords,lm(ctr ~ 
                      log(num_ads)
                      +log_imp
                      +num_word
                      +brand
                      +location
                      +log_trans
                      +avg_ad_quality
                      +log(avg_num_ads)
                      +categoryid
                      +keyword_entropy
                      ))
summary(reg)

```

**The results of the model after adding entropy as a predictor suggest that CTR does not have much of an impact on entropy. Ideally CTR should reduce for words with higher entropy or in other words higher ambiguity.**

## Final Analysis and Recommendations

As above, do an exploration and analysis of the specific keyword "target". Also, consider using other techniques that you have learned in this course (and other) to gain insights.

```{r}
summary(keywords)
keywords[keywords$query=="target",]
keywords[keywords$query=="wal mart",]
keywords[keywords$query=="walmart",]
keywords[keywords$query=="k-mart",]
keywords[keywords$query=="kmart",]
keywords[keywords$query=="costco",]

```

**If we compare Target with Walmart, Kmart and costco search terms, we observe that Target has the higehst entropy among all these terms and the least CTR. This makes sense as the ambiguity of the search term Target relative to the others results in lower CTR.**

**The recommendation would be for Target to bid on keywords which have high probabilities for topics related to retail and shopping.**


